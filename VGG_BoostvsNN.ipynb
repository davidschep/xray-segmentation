{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images is optional, CNNs are ok with large images\n",
    "SIZE_X = 501 #Resize images (height  = X, width = Y)\n",
    "SIZE_Y = 501\n",
    "\n",
    "\n",
    "#Capture training image info as a list\n",
    "train_images = []\n",
    "\n",
    "dirname = './dataset/data'  \n",
    "for fname in os.listdir(dirname):\n",
    "    img = cv2.imread(os.path.join(dirname, fname), cv2.IMREAD_GRAYSCALE)   \n",
    " \n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    train_stack = np.stack((img,)*3, axis=-1)\n",
    "    train_images.append(train_stack)\n",
    "    #train_labels.append(label)\n",
    "#Convert list to array for machine learning processing        \n",
    "dataimages = np.array(train_images)\n",
    "\n",
    "\n",
    "#Capture mask/label info as a list\n",
    "train_masks = [] \n",
    "dirname = './dataset/labels'\n",
    "for fname in os.listdir(dirname):\n",
    "    img = cv2.imread(os.path.join(dirname, fname), cv2.IMREAD_GRAYSCALE)       \n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    #mask_stack = np.stack((img,)*3, axis=-1)\n",
    "    train_masks.append(img)\n",
    "    #print(f\"Unique vlaues in labels image {np.unique(img)}\")\n",
    "        #train_labels.append(label)\n",
    "#Convert list to array for machine learning processing          \n",
    "labelimages = np.array(train_masks)\n",
    "#Cant change it here as I have the features from the VGG in the other Script\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataimages, labelimages, test_size=0.95, random_state=42)\n",
    "\n",
    "\n",
    "#Use customary x_train and y_train variables\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=3) #May not be necessary.. leftover from previous code \n",
    "y_test = np.expand_dims(y_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the feature Data from a file that I have written it to \n",
    "dataset = pd.read_feather('FeatureData.feather')\n",
    "#Get Test Train Data\n",
    "# This is so that we dont overwork our Ram\n",
    "dataset = dataset.sample(frac=0.005, random_state=1)\n",
    "\n",
    "\n",
    "#Redefine X and Y for Random Forest\n",
    "X_for_training = dataset.drop(labels = ['Label'], axis=1)\n",
    "X_for_training = X_for_training.values  #Convert to array\n",
    "Y_for_training = dataset['Label']\n",
    "Y_for_training = Y_for_training.values  #Convert to array\n",
    "mapping = {0: 0, 128: 1, 255: 2}\n",
    "\n",
    "# Vectorized mapping\n",
    "Y_for_training = np.vectorize(mapping.get)(Y_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Boost Model\n",
    "filename = 'model_XG.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 1.9666\n",
      "Epoch [1/100], Loss: 1.9666\n",
      "New best model saved with loss: 0.9300\n",
      "Epoch [2/100], Loss: 0.9300\n",
      "Epoch [3/100], Loss: 0.9843\n",
      "New best model saved with loss: 0.7471\n",
      "Epoch [4/100], Loss: 0.7471\n",
      "New best model saved with loss: 0.5740\n",
      "Epoch [5/100], Loss: 0.5740\n",
      "New best model saved with loss: 0.5705\n",
      "Epoch [6/100], Loss: 0.5705\n",
      "Epoch [7/100], Loss: 1.0469\n",
      "Epoch [8/100], Loss: 0.6151\n",
      "Epoch [9/100], Loss: 0.9585\n",
      "New best model saved with loss: 0.5323\n",
      "Epoch [10/100], Loss: 0.5323\n",
      "New best model saved with loss: 0.4428\n",
      "Epoch [11/100], Loss: 0.4428\n",
      "Epoch [12/100], Loss: 0.6773\n",
      "New best model saved with loss: 0.3780\n",
      "Epoch [13/100], Loss: 0.3780\n",
      "Epoch [14/100], Loss: 0.3907\n",
      "Epoch [15/100], Loss: 0.3796\n",
      "New best model saved with loss: 0.3322\n",
      "Epoch [16/100], Loss: 0.3322\n",
      "Epoch [17/100], Loss: 0.3548\n",
      "New best model saved with loss: 0.2675\n",
      "Epoch [18/100], Loss: 0.2675\n",
      "New best model saved with loss: 0.2630\n",
      "Epoch [19/100], Loss: 0.2630\n",
      "New best model saved with loss: 0.2256\n",
      "Epoch [20/100], Loss: 0.2256\n",
      "Epoch [21/100], Loss: 0.3423\n",
      "Epoch [22/100], Loss: 0.7796\n",
      "New best model saved with loss: 0.2205\n",
      "Epoch [23/100], Loss: 0.2205\n",
      "Epoch [24/100], Loss: 0.3317\n",
      "Epoch [25/100], Loss: 0.2243\n",
      "New best model saved with loss: 0.1773\n",
      "Epoch [26/100], Loss: 0.1773\n",
      "Epoch [27/100], Loss: 0.2345\n",
      "Epoch [28/100], Loss: 0.2368\n",
      "New best model saved with loss: 0.1548\n",
      "Epoch [29/100], Loss: 0.1548\n",
      "New best model saved with loss: 0.1325\n",
      "Epoch [30/100], Loss: 0.1325\n",
      "Epoch [31/100], Loss: 0.2147\n",
      "Epoch [32/100], Loss: 0.2536\n",
      "Epoch [33/100], Loss: 0.2660\n",
      "Epoch [34/100], Loss: 0.2588\n",
      "New best model saved with loss: 0.0922\n",
      "Epoch [35/100], Loss: 0.0922\n",
      "Epoch [36/100], Loss: 0.1918\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1994\n",
      "Epoch [39/100], Loss: 0.2568\n",
      "Epoch [40/100], Loss: 0.6055\n",
      "Epoch [41/100], Loss: 0.0983\n",
      "Epoch [42/100], Loss: 0.4361\n",
      "Epoch [43/100], Loss: 0.1918\n",
      "Epoch [44/100], Loss: 0.2499\n",
      "Epoch [45/100], Loss: 0.1266\n",
      "Epoch [46/100], Loss: 0.1081\n",
      "Epoch [47/100], Loss: 0.2274\n",
      "Epoch [48/100], Loss: 0.2701\n",
      "Epoch [49/100], Loss: 0.1757\n",
      "Epoch [50/100], Loss: 0.1745\n",
      "Epoch [51/100], Loss: 0.1136\n",
      "Epoch [52/100], Loss: 0.1208\n",
      "Epoch [53/100], Loss: 0.3331\n",
      "New best model saved with loss: 0.0680\n",
      "Epoch [54/100], Loss: 0.0680\n",
      "Epoch [55/100], Loss: 0.1640\n",
      "Epoch [56/100], Loss: 0.1125\n",
      "Epoch [57/100], Loss: 0.1708\n",
      "Epoch [58/100], Loss: 0.4024\n",
      "Epoch [59/100], Loss: 0.2130\n",
      "New best model saved with loss: 0.0502\n",
      "Epoch [60/100], Loss: 0.0502\n",
      "Epoch [61/100], Loss: 0.2310\n",
      "Epoch [62/100], Loss: 0.0601\n",
      "Epoch [63/100], Loss: 0.2636\n",
      "Epoch [64/100], Loss: 0.1598\n",
      "Epoch [65/100], Loss: 0.1449\n",
      "Epoch [66/100], Loss: 0.2552\n",
      "Epoch [67/100], Loss: 0.4429\n",
      "Epoch [68/100], Loss: 0.0977\n",
      "Epoch [69/100], Loss: 0.1105\n",
      "Epoch [70/100], Loss: 0.0768\n",
      "Epoch [71/100], Loss: 0.0935\n",
      "Epoch [72/100], Loss: 0.1358\n",
      "Epoch [73/100], Loss: 0.2373\n",
      "New best model saved with loss: 0.0298\n",
      "Epoch [74/100], Loss: 0.0298\n",
      "Epoch [75/100], Loss: 0.3226\n",
      "Epoch [76/100], Loss: 0.1285\n",
      "Epoch [77/100], Loss: 0.1825\n",
      "Epoch [78/100], Loss: 0.0519\n",
      "Epoch [79/100], Loss: 0.2945\n",
      "Epoch [80/100], Loss: 0.0917\n",
      "Epoch [81/100], Loss: 0.1287\n",
      "Epoch [82/100], Loss: 0.0434\n",
      "Epoch [83/100], Loss: 0.1373\n",
      "Epoch [84/100], Loss: 0.1035\n",
      "Epoch [85/100], Loss: 0.0829\n",
      "Epoch [86/100], Loss: 0.4999\n",
      "Epoch [87/100], Loss: 0.1939\n",
      "Epoch [88/100], Loss: 0.0511\n",
      "Epoch [89/100], Loss: 0.0757\n",
      "Epoch [90/100], Loss: 0.0475\n",
      "Epoch [91/100], Loss: 0.1993\n",
      "Epoch [92/100], Loss: 0.0832\n",
      "Epoch [93/100], Loss: 0.2026\n",
      "Epoch [94/100], Loss: 0.0918\n",
      "Epoch [95/100], Loss: 0.1402\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.1713\n",
      "Epoch [98/100], Loss: 0.1386\n",
      "Epoch [99/100], Loss: 0.0371\n",
      "Epoch [100/100], Loss: 0.0806\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neural Network\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert the data to PyTorch tensors and move them to the selected device\n",
    "X_train_tensor = torch.tensor(X_for_training).float().to(device)\n",
    "Y_train_tensor = torch.tensor(Y_for_training).long().to(device)\n",
    "\n",
    "# Create a Dataset and a DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)  # Adjust the input dimensions if needed\n",
    "        self.fc2 = nn.Linear(128, 3)   # Adjust the output dimensions based on the number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and move it to the selected device\n",
    "model = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "model.train()  # Set the model to training mode\n",
    "best_loss = float('inf')  # Initialize best loss to a large value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input features\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print and save best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), 'best_model_NN.pth')\n",
    "        print(f'New best model saved with loss: {best_loss:.4f}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model \n",
    "# Initialize the model\n",
    "loaded_model_NN = SimpleNN()\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model_NN.load_state_dict(torch.load('best_model_NN.pth'))\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loaded_model = loaded_model_NN.to(device)\n",
    "loaded_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JohannHaack\\source\\xray-segmentation\\VGG_BoostvsNN.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming X_test is your test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Convert the test data to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m first_image_batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(X_test[\u001b[39m0\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Reshape to (1, 501, 501, 3)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_feature \u001b[39m=\u001b[39m new_model\u001b[39m.\u001b[39mpredict(first_image_batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X_test_feature \u001b[39m=\u001b[39m X_test_feature\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, X_test_feature\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JohannHaack/source/xray-segmentation/VGG_BoostvsNN.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m X_test_feature_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X_test_feature)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming X_test is your test data\n",
    "# Convert the test data to a PyTorch tensor\n",
    "\n",
    "first_image_batch = np.expand_dims(X_test[0], axis=0)  # Reshape to (1, 501, 501, 3)\n",
    "X_test_feature = new_model.predict(first_image_batch)\n",
    "X_test_feature = X_test_feature.reshape(-1, X_test_feature.shape[3])\n",
    "\n",
    "\n",
    "X_test_feature_tensor = torch.tensor(X_test_feature).float()\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model and data to the device\n",
    "loaded_model_NN = loaded_model_NN.to(device)\n",
    "X_test_feature_tensor = X_test_feature_tensor.to(device)\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "loaded_model_NN.eval()\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    first_image_batch = X_test_feature_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "    outputs = loaded_model_NN(first_image_batch)\n",
    "\n",
    "# Convert outputs to probabilities using softmax (if your model doesn't already include it)\n",
    "probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "# Convert probabilities to a pandas DataFrame\n",
    "class_probabilities_df = pd.DataFrame(probabilities.cpu().numpy(), \n",
    "                                      columns=[f'Class_{i+1}_Prob' for i in range(probabilities.shape[1])])\n",
    "\n",
    "filtered_df = class_probabilities_df[(class_probabilities_df < 0.6).all(axis=1)]\n",
    "\n",
    "# Print the DataFrame\n",
    "print(filtered_df.to_string(index=False))\n",
    "\n",
    "# Convert the model output to a prediction\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "prediction_image = predicted.cpu().numpy().reshape((501, 501))\n",
    "\n",
    "# Rest of your visualization code\n",
    "original_image = np.squeeze(X_test[0])\n",
    "labeled_images = np.squeeze(y_test[0])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Labeled Image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(labeled_images, cmap='gray')\n",
    "plt.title('Labeled Image')\n",
    "\n",
    "# Prediction Image\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(prediction_image, cmap='gray')\n",
    "plt.title('Prediction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
