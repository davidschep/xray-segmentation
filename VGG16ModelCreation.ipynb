{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "from IPython.display import clear_output\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images is optional, CNNs are ok with large images\n",
    "SIZE_X = 501 #Resize images (height  = X, width = Y)\n",
    "SIZE_Y = 501\n",
    "\n",
    "\n",
    "#Capture training image info as a list\n",
    "train_images = []\n",
    "\n",
    "dirname = './dataset/data'  \n",
    "for fname in os.listdir(dirname):\n",
    "    img = cv2.imread(os.path.join(dirname, fname), cv2.IMREAD_GRAYSCALE)   \n",
    " \n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    train_stack = np.stack((img,)*3, axis=-1)\n",
    "    train_images.append(train_stack)\n",
    "    #train_labels.append(label)\n",
    "#Convert list to array for machine learning processing        \n",
    "dataimages = np.array(train_images)\n",
    "\n",
    "\n",
    "#Capture mask/label info as a list\n",
    "train_masks = [] \n",
    "dirname = './dataset/labels'\n",
    "for fname in os.listdir(dirname):\n",
    "    img = cv2.imread(os.path.join(dirname, fname), cv2.IMREAD_GRAYSCALE)       \n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    #mask_stack = np.stack((img,)*3, axis=-1)\n",
    "    train_masks.append(img)\n",
    "    #print(f\"Unique vlaues in labels image {np.unique(img)}\")\n",
    "        #train_labels.append(label)\n",
    "#Convert list to array for machine learning processing          \n",
    "labelimages = np.array(train_masks)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataimages, labelimages, test_size=0.95, random_state=42)\n",
    "\n",
    "\n",
    "#Use customary x_train and y_train variables\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=3) #May not be necessary.. leftover from previous code \n",
    "y_test = np.expand_dims(y_test, axis=3)\n",
    "\n",
    "#Load VGG16 model wothout classifier/fully connected layers\n",
    "#Load imagenet weights that we are going to use as feature generators\n",
    "#Talked to the professor and as it seems the input shape makes sure that we can actually run pooling and kernal operations\n",
    "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE_X, SIZE_Y, 3))\n",
    "\n",
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "for layer in VGG_model.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "VGG_model.summary()  #Trainable parameters will be 0\n",
    "\n",
    "#After the first 2 convolutional layers the image dimension changes. \n",
    "#So for easy comparison to Y (labels) let us only take first 2 conv layers\n",
    "#and create a new model to extract features\n",
    "#New model with only first 2 conv layers\n",
    "VGG_model = Model(inputs=VGG_model.input, outputs=VGG_model.get_layer('block1_conv2').output)\n",
    "VGG_model.summary()\n",
    "features = VGG_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the extracted features\n",
    "square = 8\n",
    "ix=1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(features[0,:,:,ix-1], cmap='gray')\n",
    "        ix +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassign 'features' as X to make it easy to follow\n",
    "X=features\n",
    "X = X.reshape(-1, X.shape[3])  #Make it compatible for Random Forest and match Y labels\n",
    "Y = y_train.reshape(-1)\n",
    "\n",
    "#Combine X and Y into a dataframe to make it easy to drop all rows with Y values 0\n",
    "\n",
    "dataset = pd.DataFrame(X)\n",
    "dataset['Label'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that we dont overwork our Ram\n",
    "dataset = dataset.sample(frac=0.005, random_state=1)\n",
    "\n",
    "#Redefine X and Y for Random Forest\n",
    "X_for_training = dataset.drop(labels = ['Label'], axis=1)\n",
    "X_for_training = X_for_training.values  #Convert to array\n",
    "Y_for_training = dataset['Label']\n",
    "Y_for_training = Y_for_training.values  #Convert to array\n",
    "mapping = {0: 0, 128: 1, 255: 2}\n",
    "# Vectorized mapping\n",
    "Y_for_training = np.vectorize(mapping.get)(Y_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "# Train the model on training data\n",
    "model.fit(X_for_training, Y_for_training) \n",
    "#Save model for future use\n",
    "filename = 'model_XG.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_for_training, dtype=tf.float32)\n",
    "Y_train_tensor = tf.convert_to_tensor(Y_for_training, dtype=tf.int64)\n",
    "\n",
    "# Create a TensorFlow Dataset and a DataLoader\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, Y_train_tensor))\n",
    "train_loader = train_dataset.shuffle(buffer_size=10000).batch(32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')  # Adjust input dimensions if needed\n",
    "        self.fc2 = layers.Dense(3)  # Adjust output dimensions based on the number of classes\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Metrics to track loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 250\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        train_step(inputs, labels)\n",
    "\n",
    "    current_loss = train_loss.result()\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        model.save_weights('NN_Model_VGG_Features.h5')\n",
    "        print(f'New best model saved with loss: {best_loss:.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {current_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to print a picture using the models we have\n",
    "filename = 'model_XG.sav'\n",
    "loaded_model_XGB = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image_batch = np.expand_dims(X_test[0], axis=0)  # Reshape to (1, 501, 501, 3)\n",
    "X_test_feature = VGG_model.predict(first_image_batch)\n",
    "X_test_feature = X_test_feature.reshape(-1, X_test_feature.shape[3])\n",
    "#These are not the exact possibilities that we want as it is the number of trees that hit as XGB is an esemble model. \n",
    "pred_probabilities = loaded_model_XGB.predict_proba(X_test_feature)\n",
    "class_probabilities_df = pd.DataFrame(pred_probabilities, \n",
    "                                      columns=[f'Class_{i+1}_Prob' for i in range(pred_probabilities.shape[1])])\n",
    "\n",
    "filtered_df = class_probabilities_df[(class_probabilities_df < 0.6).all(axis=1)]\n",
    "\n",
    "prediction = loaded_model_XGB.predict(X_test_feature)\n",
    "prediction_image = prediction.reshape((501, 501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = 501 * 501\n",
    "feature_matrix = X_test_feature.reshape((num_pixels, -1))\n",
    "\n",
    "# Convert to TensorFlow tensor\n",
    "feature_tensor = tf.convert_to_tensor(feature_matrix, dtype=tf.float32)\n",
    "\n",
    "modelNN = SimpleNN()  # Replace with your actual model architecture\n",
    "sample_input = tf.random.normal([1, 64])  # Replace 'input_shape' with the shape of your model input\n",
    "modelNN(sample_input)\n",
    "# Load the weights\n",
    "modelNN.load_weights('NN_Model_VGG_Features.h5')\n",
    "# Predict with the model\n",
    "prediction = modelNN(feature_tensor)  # model is your loaded and compiled neural network model\n",
    "prediction = tf.nn.softmax(prediction, axis=1)  # As classification dont need softmax we have it here to get probabilities and take the one with the highest argument in the end\n",
    "predicted_classes = tf.argmax(prediction, axis=1).numpy()\n",
    "\n",
    "# Reshape the predictions back into image format\n",
    "prediction_image_NN = predicted_classes.reshape((501, 501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Predictions for the first image\n",
    "original_image = np.squeeze(X_test[0])\n",
    "labeled_images = np.squeeze(y_test[0])  # Removes the singleton dimension if it exists\n",
    "\n",
    "print(original_image.shape)\n",
    "print(labeled_images.shape)\n",
    "# If X_test is a grayscale image with a shape like (n_samples, 501, 501), you might need to do:\n",
    "# original_image = X_test[0].reshape((501, 501))\n",
    "\n",
    "plt.figure(figsize=(15, 5))  # Increase the figure size for better visibility\n",
    "\n",
    "# Plot the original image\n",
    "plt.subplot(1, 4, 1)  # 1 row, 3 columns, first subplot\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Plot the labeled image\n",
    "plt.subplot(1, 4, 2)  # 1 row, 3 columns, second subplot\n",
    "plt.imshow(labeled_images, cmap='gray')  # You might not need cmap='gray' if it's a color label image\n",
    "plt.title('Labeled Image')\n",
    "\n",
    "# Plot the prediction image\n",
    "plt.subplot(1, 4, 3)  # 1 row, 3 columns, third subplot\n",
    "plt.imshow(prediction_image, cmap='gray')\n",
    "plt.title('Prediction')\n",
    "\n",
    "# Plot the prediction image\n",
    "plt.subplot(1, 4, 4)  # 1 row, 3 columns, third subplot\n",
    "plt.imshow(prediction_image_NN, cmap='gray')\n",
    "plt.title('Prediction NN')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Mean Accuracy: 0.9894159784223968\n",
      "Mean Precision: 0.9895149139805218\n",
      "Mean Recall: 0.9887047936989445\n",
      "Mean F1 Score: 0.9891009052257885\n",
      "Mean IoU: 0.9784568213564796\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "def calculate_metrics(y_true, y_pred, class_values):\n",
    "    y_pred_mapped = np.copy(y_pred)\n",
    "    for class_index, class_value in enumerate(class_values):\n",
    "        y_pred_mapped[y_pred == class_index] = class_value\n",
    "\n",
    "    y_true_flatten = y_true.flatten()\n",
    "    y_pred_flatten = y_pred_mapped.flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_flatten, y_pred_flatten)\n",
    "    \n",
    "    # For a multi-class problem, change 'binary' to 'macro' (unweighted average), 'micro' (global average), or 'weighted' (weighted average)\n",
    "    precision = precision_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    recall = recall_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    f1 = f1_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    iou = jaccard_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, iou\n",
    "\n",
    "# Initialize lists to store metric scores for each image\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "ious = []\n",
    "\n",
    "for i in range(5):  # Iterate over each image in X_test len(X_test)\n",
    "    # Prepare the image for prediction\n",
    "    image_batch = np.expand_dims(X_test[i], axis=0)\n",
    "    X_test_feature = VGG_model.predict(image_batch)\n",
    "    X_test_feature = X_test_feature.reshape(-1, X_test_feature.shape[3])\n",
    "    prediction = loaded_model_XGB.predict(X_test_feature)\n",
    "    prediction_image = prediction.reshape((501, 501))\n",
    "    \n",
    "    # Convert prediction to the same format as ground truth, if necessary\n",
    "    # Your conversion code here\n",
    "    \n",
    "    # Calculate metrics for the current prediction\n",
    "    accuracy, precision, recall, f1, iou = calculate_metrics(y_test[i], prediction_image, [0, 128, 255])\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    ious.append(iou)\n",
    "    \n",
    "    # Clear memory periodically if needed\n",
    "    if (i+1) % 10 == 0:  # Clear every 10 images, adjust based on your system's memory\n",
    "        gc.collect()\n",
    "\n",
    "# Calculate and print the mean of the metrics\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "mean_iou = np.mean(ious)\n",
    "\n",
    "print(f'Mean Accuracy: {mean_accuracy}')\n",
    "print(f'Mean Precision: {mean_precision}')\n",
    "print(f'Mean Recall: {mean_recall}')\n",
    "print(f'Mean F1 Score: {mean_f1_score}')\n",
    "print(f'Mean IoU: {mean_iou}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Neural Network Model Metrics:\n",
      "Mean Accuracy: 0.9822295528703073\n",
      "Mean Precision: 0.9815346536857545\n",
      "Mean Recall: 0.9819457644918004\n",
      "Mean F1 Score: 0.9817308463250198\n",
      "Mean IoU: 0.9641560547648471\n",
      "\n",
      "XGBoost Model Metrics:\n",
      "Mean Accuracy: 0.9894159784223968\n",
      "Mean Precision: 0.9895149139805218\n",
      "Mean Recall: 0.9887047936989445\n",
      "Mean F1 Score: 0.9891009052257885\n",
      "Mean IoU: 0.9784568213564796\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocesses the image for model prediction.\"\"\"\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def predict_with_nn(nn_model, image):\n",
    "    # Get features from VGG model\n",
    "    feature_matrix = VGG_model.predict(image)\n",
    "    feature_matrix = feature_matrix.reshape((501 * 501, -1))\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    feature_tensor = tf.convert_to_tensor(feature_matrix, dtype=tf.float32)\n",
    "    # Predict with the model\n",
    "    prediction = modelNN(feature_tensor)  # model is your loaded and compiled neural network model\n",
    "    prediction = tf.nn.softmax(prediction, axis=1)  # As classification dont need softmax we have it here to get probabilities and take the one with the highest argument in the end\n",
    "    predicted_classes = tf.argmax(prediction, axis=1).numpy()\n",
    "\n",
    "    # Reshape the predictions back into image format\n",
    "    prediction_image_NN = predicted_classes.reshape((501, 501))\n",
    "    return prediction_image_NN\n",
    "\n",
    "def predict_with_xgb( xgb_model, image):\n",
    "    feature_matrix = VGG_model.predict(image)\n",
    "    feature_matrix = feature_matrix.reshape(-1, feature_matrix.shape[3])\n",
    "    prediction = xgb_model.predict(feature_matrix)\n",
    "    prediction_image = prediction.reshape((501, 501))\n",
    "    return prediction_image\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred, class_values):\n",
    "    y_pred_mapped = np.copy(y_pred)\n",
    "    for class_index, class_value in enumerate(class_values):\n",
    "        y_pred_mapped[y_pred == class_index] = class_value\n",
    "\n",
    "    y_true_flatten = y_true.flatten()\n",
    "    y_pred_flatten = y_pred_mapped.flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_flatten, y_pred_flatten)\n",
    "    \n",
    "    # For a multi-class problem, change 'binary' to 'macro' (unweighted average), 'micro' (global average), or 'weighted' (weighted average)\n",
    "    precision = precision_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    recall = recall_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    f1 = f1_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    iou = jaccard_score(y_true_flatten, y_pred_flatten, average='macro', labels=class_values)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, iou\n",
    "\n",
    "def compute_and_print_mean_metrics(metrics):\n",
    "    \"\"\"Computes and prints the mean of the given metrics.\"\"\"\n",
    "    mean_metrics = np.mean(metrics, axis=0)\n",
    "    metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"IoU\"]\n",
    "\n",
    "    for name, mean_metric in zip(metric_names, mean_metrics):\n",
    "        print(f'Mean {name}: {mean_metric}')\n",
    "\n",
    "# Initialize lists to store metric scores for each image for both models\n",
    "nn_metrics, xgb_metrics = [], []\n",
    "\n",
    "for i in range(5):\n",
    "    image = preprocess_image(X_test[i])\n",
    "\n",
    "    # Predict with Neural Network and XGBoost models\n",
    "    xgb_prediction = predict_with_xgb(loaded_model_XGB, image)\n",
    "\n",
    "    nn_prediction = predict_with_nn(modelNN, image)\n",
    "\n",
    "    # Convert prediction to the same format as ground truth, if necessary\n",
    "    # Example: nn_prediction_converted = convert_format(nn_prediction)\n",
    "    # Example: xgb_prediction_converted = convert_format(xgb_prediction)\n",
    "\n",
    "    # Calculate metrics\n",
    "    nn_metrics.append(calculate_metrics(y_test[i].flatten(), nn_prediction.flatten(),[0, 128, 255]))\n",
    "    xgb_metrics.append(calculate_metrics(y_test[i].flatten(), xgb_prediction.flatten(),[0, 128, 255]))\n",
    "\n",
    "    # Memory management\n",
    "    if (i + 1) % 10 == 0:  # Clear memory every 10 iterations\n",
    "        gc.collect()\n",
    "\n",
    "# Print mean metrics for both models\n",
    "print(\"Neural Network Model Metrics:\")\n",
    "compute_and_print_mean_metrics(nn_metrics)\n",
    "\n",
    "print(\"\\nXGBoost Model Metrics:\")\n",
    "compute_and_print_mean_metrics(xgb_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
